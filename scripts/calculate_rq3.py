import pandas as pd
import networkx as nx
import numpy as np
from collections import defaultdict
import logging
import time
from datetime import datetime
import psutil
import os

# Configurar logging
def setup_logging():
    """Configura logging com diferentes nÃ­veis."""
    log_filename = f"logs/rq3_full_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # Criar diretÃ³rio de logs se nÃ£o existir
    os.makedirs('logs', exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def log_memory_usage(logger, step_name):
    """Log do uso de memÃ³ria atual."""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    memory_mb = memory_info.rss / 1024 / 1024
    logger.info(f"ğŸ“Š {step_name} - MemÃ³ria: {memory_mb:.2f} MB")

def load_full_collaboration_data(logger):
    """Carregar dados completos de colaboraÃ§Ã£o."""
    logger.info("ğŸ”„ Carregando dados completos de colaboraÃ§Ã£o...")
    
    try:
        # Carregar dados de paÃ­ses
        users_countries_df = pd.read_csv('csv/users_countries.csv')
        logger.info(f"  ğŸ“Š Carregados dados de {len(users_countries_df)} usuÃ¡rios com paÃ­ses")
        
        # Carregar dados de mÃ©tricas
        users_metrics_df = pd.read_csv('csv/users_metrics.csv')
        logger.info(f"  ï¿½ Carregados dados de mÃ©tricas de {len(users_metrics_df)} usuÃ¡rios")
        
        # Combinar os datasets - preservando colunas de paÃ­s
        combined_df = pd.merge(users_countries_df, users_metrics_df, on='login', how='inner', suffixes=('', '_dup'))
        
        # Usar a coluna de paÃ­s original (sem sufixo)
        combined_df = combined_df.drop([col for col in combined_df.columns if col.endswith('_dup')], axis=1)
        
        logger.info(f"  ğŸ”— Dados combinados: {len(combined_df)} usuÃ¡rios com paÃ­ses e mÃ©tricas")
        
        # Classificar paÃ­ses
        emerging_countries = {
            'brazil', 'india', 'china', 'south africa', 'russia', 'mexico', 'indonesia', 
            'turkey', 'thailand', 'malaysia', 'philippines', 'vietnam', 'argentina',
            'colombia', 'chile', 'peru', 'ukraine', 'romania', 'bulgaria', 'croatia',
            'poland', 'czech republic', 'hungary', 'egypt', 'nigeria', 'kenya', 'ghana'
        }
        
        combined_df['country_type'] = combined_df['country'].str.lower().apply(
            lambda x: 'emerging' if x in emerging_countries else 'developed'
        )
        
        logger.info(f"  ğŸŒ ClassificaÃ§Ã£o de paÃ­ses:")
        logger.info(f"    - PaÃ­ses emergentes: {(combined_df['country_type'] == 'emerging').sum()} usuÃ¡rios")
        logger.info(f"    - PaÃ­ses desenvolvidos: {(combined_df['country_type'] == 'developed').sum()} usuÃ¡rios")
        
        return combined_df
        
    except Exception as e:
        logger.error(f"âŒ Erro ao carregar dados: {e}")
        raise

def create_optimized_collaboration_network(df, logger):
    """
    Cria rede de colaboraÃ§Ã£o otimizada para processar todos os dados.
    """
    logger.info("ğŸŒ Iniciando criaÃ§Ã£o da rede de colaboraÃ§Ã£o...")
    start_time = time.time()
    
    G = nx.Graph()
    
    # AnÃ¡lise inicial dos repositÃ³rios
    repo_stats = df.groupby('repo_name').size().reset_index(name='contributors')
    
    logger.info(f"ğŸ“Š EstatÃ­sticas dos repositÃ³rios:")
    logger.info(f"  - Total de repositÃ³rios: {len(repo_stats)}")
    logger.info(f"  - MÃ©dia de contribuidores por repo: {repo_stats['contributors'].mean():.2f}")
    logger.info(f"  - Mediana de contribuidores: {repo_stats['contributors'].median():.2f}")
    logger.info(f"  - MÃ¡ximo de contribuidores: {repo_stats['contributors'].max()}")
    
    # SEM LIMITE - incluir TODOS os repositÃ³rios para anÃ¡lise completa
    logger.info(f"ğŸŒ Incluindo TODOS os repositÃ³rios (sem filtros)")
    logger.info(f"  - RepositÃ³rios processados: {len(repo_stats)} (100%)")
    logger.info(f"  - Desenvolvedores processados: {len(df)} (100%)")
    
    # Usar todos os dados sem filtrar
    df_filtered = df
    
    # Agrupar por repositÃ³rio
    repo_groups = df_filtered.groupby('repo_name')['login'].apply(list).to_dict()
    
    logger.info(f"ğŸ”— Processando {len(repo_groups)} repositÃ³rios para criar arestas...")
    
    # Criar arestas com batches otimizados para volume maior
    edge_weights = defaultdict(int)
    batch_size = 50  # Batch maior para processar mais repositÃ³rios
    processed_repos = 0
    
    for repo_name, contributors in repo_groups.items():
        if len(contributors) > 1:
            # Conectar todos os pares de contribuidores
            for i in range(len(contributors)):
                for j in range(i + 1, len(contributors)):
                    user1, user2 = contributors[i], contributors[j]
                    # Ordenar para evitar duplicatas (user1, user2) vs (user2, user1)
                    if user1 > user2:
                        user1, user2 = user2, user1
                    edge_weights[(user1, user2)] += 1
        
        processed_repos += 1
        if processed_repos % batch_size == 0:
            logger.info(f"  - Processados {processed_repos}/{len(repo_groups)} repositÃ³rios ({processed_repos/len(repo_groups)*100:.1f}%)")
            log_memory_usage(logger, f"Batch {processed_repos//batch_size}")
    
    # Adicionar arestas ao grafo
    logger.info(f"ğŸ”— Adicionando {len(edge_weights)} arestas ao grafo...")
    start_edges = time.time()
    
    for (user1, user2), weight in edge_weights.items():
        G.add_edge(user1, user2, weight=weight)
    
    edges_time = time.time() - start_edges
    total_time = time.time() - start_time
    
    logger.info(f"âœ… Rede criada em {total_time:.2f}s (arestas: {edges_time:.2f}s)")
    logger.info(f"ğŸ“Š Rede final: {G.number_of_nodes()} nÃ³s, {G.number_of_edges()} arestas")
    log_memory_usage(logger, "Rede criada")
    
    # AnÃ¡lise da conectividade
    components = list(nx.connected_components(G))
    largest_component_size = max(len(comp) for comp in components) if components else 0
    
    logger.info(f"ğŸ”— AnÃ¡lise de conectividade:")
    logger.info(f"  - Componentes conectados: {len(components)}")
    logger.info(f"  - Maior componente: {largest_component_size} nÃ³s ({largest_component_size/G.number_of_nodes()*100:.1f}%)")
    
    return G

def calculate_centrality_metrics_parallel(G, logger):
    """Calcula mÃ©tricas de centralidade de forma otimizada e paralela quando possÃ­vel."""
    logger.info("ğŸ“ Iniciando cÃ¡lculo de mÃ©tricas de centralidade...")
    
    centrality_metrics = {}
    
    # 1. Degree Centrality (rÃ¡pido)
    logger.info("  ğŸ”¢ Calculando Degree Centrality...")
    start_time = time.time()
    centrality_metrics['degree'] = nx.degree_centrality(G)
    logger.info(f"    âœ… ConcluÃ­do em {time.time() - start_time:.2f}s")
    
    # 2. Betweenness Centrality (usar aproximaÃ§Ã£o para redes grandes)
    logger.info("  ğŸŒ‰ Calculando Betweenness Centrality...")
    start_time = time.time()
    
    if G.number_of_nodes() > 5000:
        # Usar aproximaÃ§Ã£o com amostra maior para maior precisÃ£o
        k = min(500, G.number_of_nodes() // 10)
        logger.info(f"    ğŸ“Š Usando aproximaÃ§Ã£o com amostra de {k} nÃ³s")
        centrality_metrics['betweenness'] = nx.betweenness_centrality(G, k=k, normalized=True)
    else:
        centrality_metrics['betweenness'] = nx.betweenness_centrality(G, normalized=True)
    
    logger.info(f"    âœ… ConcluÃ­do em {time.time() - start_time:.2f}s")
    log_memory_usage(logger, "Betweenness centrality")
    
    # 3. Closeness Centrality (calcular apenas para maior componente)
    logger.info("  ğŸ“ Calculando Closeness Centrality...")
    start_time = time.time()
    
    # Encontrar maior componente conectado
    components = list(nx.connected_components(G))
    if components:
        largest_cc = max(components, key=len)
        logger.info(f"    ğŸ”— Maior componente: {len(largest_cc)} nÃ³s")
        
        if len(largest_cc) > 10000:
            # Para componentes muito grandes, usar amostra
            sample_size = min(5000, len(largest_cc))
            sampled_nodes = np.random.choice(list(largest_cc), sample_size, replace=False)
            subgraph = G.subgraph(sampled_nodes)
            logger.info(f"    ğŸ“Š Usando amostra de {sample_size} nÃ³s do maior componente")
        else:
            subgraph = G.subgraph(largest_cc)
        
        closeness_partial = nx.closeness_centrality(subgraph)
        centrality_metrics['closeness'] = {node: closeness_partial.get(node, 0.0) for node in G.nodes()}
    else:
        centrality_metrics['closeness'] = {node: 0.0 for node in G.nodes()}
    
    logger.info(f"    âœ… ConcluÃ­do em {time.time() - start_time:.2f}s")
    
    # 4. Eigenvector Centrality (com fallback para PageRank)
    logger.info("  ğŸ¯ Calculando Eigenvector Centrality...")
    start_time = time.time()
    
    try:
        centrality_metrics['eigenvector'] = nx.eigenvector_centrality(G, max_iter=1000, tol=1e-4)
        logger.info(f"    âœ… Eigenvector centrality concluÃ­do")
    except (nx.PowerIterationFailedConvergence, nx.NetworkXError) as e:
        logger.warning(f"    âš ï¸ Eigenvector centrality falhou: {e}")
        logger.info(f"    ğŸ”„ Usando PageRank como fallback...")
        centrality_metrics['eigenvector'] = nx.pagerank(G, max_iter=1000, tol=1e-4)
        logger.info(f"    âœ… PageRank concluÃ­do como proxy")
    except Exception as e:
        logger.error(f"    âŒ Erro crÃ­tico: {e}")
        centrality_metrics['eigenvector'] = {node: 0.0 for node in G.nodes()}
    
    logger.info(f"    âœ… ConcluÃ­do em {time.time() - start_time:.2f}s")
    log_memory_usage(logger, "Eigenvector centrality")
    
    return centrality_metrics

def calculate_structural_holes_optimized(G, logger):
    """Calcula structural holes de forma otimizada."""
    logger.info("ğŸ•³ï¸ Calculando Structural Holes...")
    start_time = time.time()
    
    structural_holes = {}
    total_nodes = G.number_of_nodes()
    
    if total_nodes > 10000:
        logger.info("  ğŸ“Š Rede grande detectada, usando algoritmo otimizado...")
        # Para redes grandes, usar effective size como proxy
        batch_size = 1000
        processed = 0
        
        for node in G.nodes():
            neighbors = list(G.neighbors(node))
            if len(neighbors) <= 1:
                structural_holes[node] = 0.0
            else:
                # Effective size simplificado
                redundancy = 0
                for neighbor in neighbors:
                    common_neighbors = len(set(G.neighbors(neighbor)) & set(neighbors))
                    redundancy += common_neighbors / len(neighbors) if neighbors else 0
                
                effective_size = len(neighbors) - redundancy
                structural_holes[node] = effective_size / len(neighbors)
            
            processed += 1
            if processed % batch_size == 0:
                logger.info(f"    ğŸ“ˆ Processados {processed}/{total_nodes} nÃ³s ({processed/total_nodes*100:.1f}%)")
                
    else:
        logger.info("  ğŸ” Usando algoritmo completo de Burt constraint...")
        # Algoritmo completo para redes menores
        constraint = {}
        
        for node in G.nodes():
            neighbors = list(G.neighbors(node))
            if len(neighbors) <= 1:
                constraint[node] = 1.0
            else:
                total_constraint = 0
                degree = G.degree(node)
                
                for neighbor in neighbors:
                    direct_prop = 1.0 / degree
                    common_neighbors = set(G.neighbors(neighbor)) & set(neighbors)
                    indirect_prop = sum(1.0 / (G.degree(neighbor) * G.degree(common)) 
                                      for common in common_neighbors if common != node)
                    
                    node_constraint = (direct_prop + indirect_prop) ** 2
                    total_constraint += node_constraint
                
                constraint[node] = total_constraint
        
        # Converter para structural holes
        max_constraint = max(constraint.values()) if constraint else 1.0
        for node, c in constraint.items():
            structural_holes[node] = 1.0 - (c / max_constraint)
    
    logger.info(f"  âœ… Structural holes concluÃ­do em {time.time() - start_time:.2f}s")
    log_memory_usage(logger, "Structural holes")
    
    return structural_holes

def analyze_absence_impact_full(df, logger):
    """AnÃ¡lise completa de impacto da ausÃªncia."""
    logger.info("â° Analisando impacto da ausÃªncia...")
    start_time = time.time()
    
    absence_impact = {}
    threshold_days = 30
    
    logger.info(f"  ğŸ“Š Analisando {df['login'].nunique()} usuÃ¡rios Ãºnicos...")
    
    for login in df['login'].unique():
        user_data = df[df['login'] == login]
        avg_time = user_data['avg_time_to_merge'].mean()
        pr_count = user_data['prs_opened'].sum()
        commits = user_data['commits_total'].sum()
        
        # Calcular impacto baseado em mÃºltiplos fatores
        if pd.isna(avg_time) or avg_time == 0:
            impact = 0.0
        else:
            # Impacto base do tempo
            time_impact = min(avg_time / threshold_days, 2.0)  # Cap em 2.0
            
            # Ajustar por volume de atividade
            activity_weight = min((pr_count + commits/10) / 10, 1.5)  # Cap em 1.5
            
            # Impacto final normalizado
            impact = min(time_impact * activity_weight, 1.0)
        
        absence_impact[login] = impact
    
    logger.info(f"  âœ… Impacto da ausÃªncia concluÃ­do em {time.time() - start_time:.2f}s")
    
    # EstatÃ­sticas do impacto
    impacts = list(absence_impact.values())
    logger.info(f"  ğŸ“ˆ EstatÃ­sticas de impacto:")
    logger.info(f"    - MÃ©dia: {np.mean(impacts):.4f}")
    logger.info(f"    - Mediana: {np.median(impacts):.4f}")
    logger.info(f"    - Alto impacto (>0.8): {sum(1 for i in impacts if i > 0.8)} usuÃ¡rios")
    
    return absence_impact

def create_full_rq3_analysis():
    """FunÃ§Ã£o principal para anÃ¡lise completa da RQ3."""
    
    # Setup logging
    logger = setup_logging()
    logger.info("ğŸš€ Iniciando anÃ¡lise COMPLETA da RQ3 - Centralidade na Rede")
    logger.info("=" * 80)
    
    overall_start = time.time()
    
    try:
        # 1. Carregar dados completos
        df = load_full_collaboration_data(logger)
        
        # 2. Criar rede de colaboraÃ§Ã£o
        G = create_optimized_collaboration_network(df, logger)
        
        if G.number_of_nodes() == 0:
            logger.error("âŒ Rede vazia criada. Abortando anÃ¡lise.")
            return None
        
        # 3. Calcular mÃ©tricas de centralidade
        centrality_metrics = calculate_centrality_metrics_parallel(G, logger)
        
        # 4. Calcular structural holes
        structural_holes = calculate_structural_holes_optimized(G, logger)
        
        # 5. AnÃ¡lise de impacto da ausÃªncia
        absence_impact = analyze_absence_impact_full(df, logger)
        
        # 6. Processar mÃ©tricas por usuÃ¡rio com salvamento incremental
        logger.info("ğŸ‘¥ Processando mÃ©tricas por usuÃ¡rio...")
        start_time = time.time()
        
        all_users = df['login'].unique()
        
        # ConfiguraÃ§Ã£o para salvamento incremental
        batch_save_size = 50
        save_counter = 0
        metrics_data = []
        
        for i, user in enumerate(all_users):
            user_data = df[df['login'] == user].iloc[0].to_dict()
            
            # ClassificaÃ§Ã£o de perfil
            prs = user_data.get('prs_opened', 0) + user_data.get('prs_merged', 0)
            commits = user_data.get('commits_total', 0)
            reviews = user_data.get('reviews_submitted', 0)
            permission = user_data.get('permission_level', '').lower()
            
            if permission in ['admin', 'maintain', 'owner']:
                profile = 'Core Developer / Maintainer'
            elif prs >= 3 or commits >= 5 or reviews >= 3:
                profile = 'Peripheral Developer'
            else:
                profile = 'One-time Contributor / Newcomer'
            
            metrics_data.append({
                'login': user,
                'degree_centrality': centrality_metrics['degree'].get(user, 0.0),
                'betweenness_centrality': centrality_metrics['betweenness'].get(user, 0.0),
                'closeness_centrality': centrality_metrics['closeness'].get(user, 0.0),
                'eigenvector_centrality': centrality_metrics['eigenvector'].get(user, 0.0),
                'structural_hole_spanners': structural_holes.get(user, 0.0),
                'developer_profile': profile,
                'is_maintainer': 1 if permission in ['admin', 'maintain', 'owner'] else 0,
                'absence_impact': absence_impact.get(user, 0.0),
                'degree': G.degree(user) if G.has_node(user) else 0,
                'is_isolated': user not in G.nodes() or G.degree(user) == 0,
                'country': user_data.get('country', ''),
                'prs_opened': user_data.get('prs_opened', 0),
                'commits_total': user_data.get('commits_total', 0),
                'reviews_submitted': user_data.get('reviews_submitted', 0),
            })
            
            # Salvamento incremental a cada 50 usuÃ¡rios
            if (i + 1) % batch_save_size == 0 or (i + 1) == len(all_users):
                save_counter += 1
                partial_df = pd.DataFrame(metrics_data)
                
                # Salvar arquivo parcial
                partial_filename = f'csv/rq3_metrics_batch_{save_counter:03d}.csv'
                partial_df.to_csv(partial_filename, index=False)
                
                logger.info(f"ğŸ“„ Batch {save_counter} salvo: {partial_filename}")
                logger.info(f"   ğŸ“ˆ Processados {i+1}/{len(all_users)} usuÃ¡rios ({(i+1)/len(all_users)*100:.1f}%)")
                
                # Limpar dados para prÃ³ximo batch (economizar memÃ³ria)
                metrics_data = []
                
                # Log de memÃ³ria a cada 1000 usuÃ¡rios
                if (i + 1) % 1000 == 0:
                    log_memory_usage(logger, f"UsuÃ¡rio milestone {(i+1)//1000}k")
        
        process_time = time.time() - start_time
        logger.info(f"  âœ… Processamento concluÃ­do em {process_time:.2f}s")
        logger.info(f"  ğŸ“„ Total de batches salvos: {save_counter}")
        
        # 7. Consolidar todos os arquivos em um final
        logger.info("ğŸ’¾ Consolidando arquivos parciais...")
        
        # Ler todos os arquivos parciais
        all_batches = []
        for batch_num in range(1, save_counter + 1):
            batch_file = f'csv/rq3_metrics_batch_{batch_num:03d}.csv'
            if os.path.exists(batch_file):
                batch_df = pd.read_csv(batch_file)
                all_batches.append(batch_df)
        
        # Concatenar todos os batches
        metrics_df = pd.concat(all_batches, ignore_index=True)
        
        output_file = 'csv/network_metrics.csv'
        metrics_df.to_csv(output_file, index=False)
        
        # Limpar arquivos parciais para manter organizaÃ§Ã£o
        logger.info("ğŸ§¹ Limpando arquivos parciais...")
        for batch_num in range(1, save_counter + 1):
            batch_file = f'csv/rq3_metrics_batch_{batch_num:03d}.csv'
            if os.path.exists(batch_file):
                os.remove(batch_file)
        logger.info(f"   âœ… Removidos {save_counter} arquivos parciais")
        
        # 8. RelatÃ³rio final
        total_time = time.time() - overall_start
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ ANÃLISE COMPLETA CONCLUÃDA!")
        logger.info(f"â±ï¸ Tempo total: {total_time:.2f}s ({total_time/60:.1f}min)")
        logger.info(f"ğŸ“ Arquivo salvo: {output_file}")
        logger.info(f"ğŸ‘¥ Total de usuÃ¡rios: {len(metrics_df)}")
        logger.info(f"ğŸŒ Rede final: {G.number_of_nodes()} nÃ³s, {G.number_of_edges()} arestas")
        log_memory_usage(logger, "Final")
        
        # EstatÃ­sticas finais
        logger.info("\nğŸ“Š ESTATÃSTICAS FINAIS:")
        
        profile_counts = metrics_df['developer_profile'].value_counts()
        logger.info("ğŸ‘¥ Perfis de desenvolvedores:")
        for profile, count in profile_counts.items():
            logger.info(f"  - {profile}: {count} ({count/len(metrics_df)*100:.1f}%)")
        
        maintainers = metrics_df['is_maintainer'].sum()
        logger.info(f"ğŸ‘‘ Maintainers: {maintainers} ({maintainers/len(metrics_df)*100:.2f}%)")
        
        # Top performers
        logger.info("\nğŸ† TOP PERFORMERS:")
        
        top_betweenness = metrics_df.nlargest(5, 'betweenness_centrality')
        logger.info("ğŸŒ‰ Top 5 Betweenness Centrality:")
        for _, row in top_betweenness.iterrows():
            logger.info(f"  - {row['login']} ({row['country']}): {row['betweenness_centrality']:.6f}")
        
        top_spanners = metrics_df.nlargest(5, 'structural_hole_spanners')
        logger.info("ğŸ•³ï¸ Top 5 Structural Hole Spanners:")
        for _, row in top_spanners.iterrows():
            logger.info(f"  - {row['login']} ({row['country']}): {row['structural_hole_spanners']:.4f}")
        
        logger.info("=" * 80)
        
        return metrics_df
        
    except Exception as e:
        logger.error(f"âŒ Erro crÃ­tico na anÃ¡lise: {e}")
        logger.exception("Detalhes do erro:")
        return None

if __name__ == "__main__":
    result = create_full_rq3_analysis()
    if result is not None:
        print(f"\nâœ… AnÃ¡lise concluÃ­da com sucesso! Dataset com {len(result)} usuÃ¡rios criado.")
        print("ğŸ“ Arquivo: csv/network_metrics.csv")
        print("ğŸ“‹ Logs salvos em: logs/")
    else:
        print("âŒ AnÃ¡lise falhou. Verifique os logs para detalhes.")
